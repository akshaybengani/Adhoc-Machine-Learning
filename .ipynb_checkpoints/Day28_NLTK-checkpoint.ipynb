{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7fm6KOT-L5D"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZaW2zy_w-d-9",
    "outputId": "fa0ab646-37e0-4327-ac3c-9334db874701"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vCAYT7mZ-1Fv",
    "outputId": "680cf25d-9743-464f-ac32-983d6af1d084"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XDfF3YRc_qFb",
    "outputId": "092d5c7a-8315-4c08-dadf-24121173a761"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-dlu1N6_vNO"
   },
   "outputs": [],
   "source": [
    "# Web Scrapping with NLP\n",
    "url = 'https://php.net/'\n",
    "import requests, bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M_20ARjfDXOd",
    "outputId": "0740853f-2251-4662-cf47-0c21078b17da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ConnectTimeout',\n",
       " 'ConnectionError',\n",
       " 'DependencyWarning',\n",
       " 'FileModeWarning',\n",
       " 'HTTPError',\n",
       " 'NullHandler',\n",
       " 'PreparedRequest',\n",
       " 'ReadTimeout',\n",
       " 'Request',\n",
       " 'RequestException',\n",
       " 'RequestsDependencyWarning',\n",
       " 'Response',\n",
       " 'Session',\n",
       " 'Timeout',\n",
       " 'TooManyRedirects',\n",
       " 'URLRequired',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__build__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__cake__',\n",
       " '__copyright__',\n",
       " '__description__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__title__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " '_check_cryptography',\n",
       " '_internal_utils',\n",
       " 'adapters',\n",
       " 'api',\n",
       " 'auth',\n",
       " 'certs',\n",
       " 'chardet',\n",
       " 'check_compatibility',\n",
       " 'codes',\n",
       " 'compat',\n",
       " 'cookies',\n",
       " 'delete',\n",
       " 'exceptions',\n",
       " 'get',\n",
       " 'head',\n",
       " 'hooks',\n",
       " 'logging',\n",
       " 'models',\n",
       " 'options',\n",
       " 'packages',\n",
       " 'patch',\n",
       " 'post',\n",
       " 'put',\n",
       " 'request',\n",
       " 'session',\n",
       " 'sessions',\n",
       " 'status_codes',\n",
       " 'structures',\n",
       " 'urllib3',\n",
       " 'utils',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQx1FNSUDdtY"
   },
   "outputs": [],
   "source": [
    "webdata = requests.get(url)\n",
    "\n",
    "# Response code of HTTP protocol\n",
    "# Where 200 means url is ok\n",
    "# 302 means url is redirected\n",
    "# 404 means url not found\n",
    "# 505 means url is not permitting for web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4snbW-vDq-U"
   },
   "outputs": [],
   "source": [
    "# Now getting actual data of any website or URL\n",
    "htmldata = webdata.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yaTVKQj2DzRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tNnyw7qF4TJ"
   },
   "source": [
    "* We need a parser for extracting data from the html code\n",
    "* As such we have a liberary called BeautifulSoup for parsing the HTML data to meaningfull data\n",
    "* To install Beautiful Soup use ```pip3 install bs4```\n",
    "* Then we need a parser as such we will use ```pip3 install html5lib```\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SubgPhGGEr_q"
   },
   "outputs": [],
   "source": [
    "# To import Beautiful soup use\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrotFz0MFUK-"
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-673fa00078c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtmldata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html5lib'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This is html data and we use the parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\drvirus\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                     % \",\".join(features))\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(htmldata,'html5lib') # This is html data and we use the parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5wmNKDKEGd-m",
    "outputId": "3c4d2b6d-545d-4097-c836-5959263ba6e4"
   },
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZgTTE6zGe9u"
   },
   "outputs": [],
   "source": [
    "# Finding all data\n",
    "clean_data = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sNYdKPT6Gxbj",
    "outputId": "5860aa63-b6df-4a94-d13f-69922986dae0"
   },
   "outputs": [],
   "source": [
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MacI5saG3sB"
   },
   "outputs": [],
   "source": [
    "# Saving data permanently\n",
    "with open('mywebdata.txt','w+') as f:\n",
    "  f.write(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCuJown1HlKz"
   },
   "outputs": [],
   "source": [
    "mydata = str(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_rQ0gBgIYFf"
   },
   "outputs": [],
   "source": [
    "datainlist = mydata.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0PTiERzJUUd"
   },
   "outputs": [],
   "source": [
    "# Now importing pandas and NLTK\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# If in case sometimes matplotlib didnt show the data due to not supporting by pandas and nltk then use this\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XKz5TSoNJV59",
    "outputId": "17554f08-6d0b-4c4d-83c9-70ac956af030"
   },
   "outputs": [],
   "source": [
    "[i for i in dir(nltk) if 'Freq' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gT_tlLE9J_UC"
   },
   "outputs": [],
   "source": [
    "# FreqDist is a function which short the data in terms of count and prepare a list with count of words\n",
    "nlpdata = nltk.FreqDist(datainlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "DH56H3iEKJ_i",
    "outputId": "d6638d95-3a4c-4316-cfb2-563387e39ee8"
   },
   "outputs": [],
   "source": [
    "# NLTK by default comes with matplotlib so you dont need that\n",
    "nlpdata.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFnyo8QVKfL9"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLUWHsl0LS0a"
   },
   "outputs": [],
   "source": [
    "# Removig Stopwords\n",
    "removedata = [i for i in newdata1 if i.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1uEK2qlNLmh0",
    "outputId": "4a1f0c85-3b72-409d-dbea-17bd93caa7a0"
   },
   "outputs": [],
   "source": [
    "removedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "colab_type": "code",
    "id": "BNDdHGGTL0Sa",
    "outputId": "38da470b-3c13-429e-ba39-362dbbfb1348"
   },
   "outputs": [],
   "source": [
    "# This is the data which does not contains any stop words\n",
    "nlpremove = nltk.FreqDist(removedata)\n",
    "nlpremove.plot(20)\n",
    "nlpdata.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_z-5tcEMB1r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S3lY1WrN-_g"
   },
   "source": [
    "# Task\n",
    "* Take picture of a newspaper\n",
    "* Extract Text\n",
    "* Apply NLTK\n",
    "* Remove stop words\n",
    "* Plot top 10 frequency graphs\n",
    "* Make sentence from words\n",
    "* Scrap a data from wikipedia then apply tokenization then apply stopwords, then apply stemming and save the data in a file.\n",
    "\n",
    "## NLTK Tokenization\n",
    "when we need to divide data in small chunks like (Sentence, Word, Character)\n",
    "The dividing of a big data in small chunks is called tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sS2ybEC9OM6_"
   },
   "outputs": [],
   "source": [
    "msg = ''' Hi my name is Akshay Bengani!\n",
    "I just published one more google action name as Akshay Bengani\n",
    "my google action is like my own chatbot.\n",
    "I searched on google using spiders and crawlers?\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSg91fWjQf0M"
   },
   "outputs": [],
   "source": [
    "# import tokenize\n",
    "from nltk.tokenize import sent_tokenize # This is for using Sentence\n",
    "from nltk.tokenize import word_tokenize # Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2iH9CN4PXWli"
   },
   "outputs": [],
   "source": [
    "# sentence Tokenize\n",
    "sent = sent_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jlJvUgVxXcd6",
    "outputId": "810721da-5093-4326-881a-1d74329b4d46"
   },
   "outputs": [],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbQBj9iIXd1P"
   },
   "outputs": [],
   "source": [
    "# Now tokenizing words\n",
    "word = word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e_gTygNXX7zU",
    "outputId": "d945abd3-715d-4357-f00c-22c8597c6737"
   },
   "outputs": [],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMCJXiLwX9hQ"
   },
   "outputs": [],
   "source": [
    "# Stopword Remover\n",
    "removedwords = [i for i in word if i.lower not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HmGXhG0IYi1A",
    "outputId": "0d8c76ab-b38e-4e39-d87e-8265e868388a"
   },
   "outputs": [],
   "source": [
    "len(removedwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "q1lTNDf7YnZp",
    "outputId": "0f978fe3-f1e9-4053-c3f5-5d57b604a5de"
   },
   "outputs": [],
   "source": [
    "nlpword = nltk.FreqDist(removedwords)\n",
    "nlpword.plot() # plot in actual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEfwBLEOZCXs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7euyYKfQa0oc"
   },
   "source": [
    "## Stemming and Lemmatization\n",
    "When we have a large amount of data and we find  a keyword in it, it can suggest it spam or hamm. This system is called stemming<br>\n",
    "Example go,going,gone,goi etc all are waste for me I don't want this.\n",
    "### Use Cases of Stemming\n",
    "Spam Filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEgKYuxbdLTX"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer # This is for stemming\n",
    "from nltk.stem import WordNetLemmatizer # This is for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlT4Gb8pdYRK"
   },
   "outputs": [],
   "source": [
    "portstem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "N0wKC_09dZI6",
    "outputId": "8b48025f-9024-4d07-903f-a568bddc10f9"
   },
   "outputs": [],
   "source": [
    "# word suppy\n",
    "for i in ['going','go','goes','govinda','golu','intel','intelligent','intelligently','ashutosh','ashu','redashu']:\n",
    "  print(portstem.stem(i))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0h6-F5Cxeggc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day28 NLTK.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
